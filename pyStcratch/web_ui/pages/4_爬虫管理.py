"""
çˆ¬è™«ç®¡ç†é¡µ - è§¦å‘å’Œç›‘æ§çˆ¬è™«ä»»åŠ¡
"""
import streamlit as st
from utils.api import backend_api, database_api
from utils.config import SOURCES, SOURCES_LABELS
from components.charts import crawl_logs_chart


def show():
    """Display the crawler management page."""
    st.title("ğŸ•·ï¸ çˆ¬è™«ç®¡ç†")

    tab1, tab2 = st.tabs(["è§¦å‘çˆ¬è™«", "æ‰§è¡Œè®°å½•"])

    with tab1:
        _show_crawl_trigger()

    with tab2:
        _show_crawl_logs()


def _show_crawl_trigger():
    """Show crawl trigger interface."""
    st.subheader("æ‰‹åŠ¨è§¦å‘çˆ¬è™«")

    # Source selection
    st.write("**é€‰æ‹©æ•°æ®æº**")
    all_sources = ["all"] + list(SOURCES)

    source_col1, source_col2 = st.columns(2)
    with source_col1:
        selected_source = st.selectbox(
            "æ•°æ®æº",
            all_sources,
            format_func=lambda x: "å…¨éƒ¨" if x == "all" else SOURCES_LABELS.get(x, x),
            index=0
        )

    # Max pages
    col1, col2 = st.columns(2)
    with col1:
        max_pages = st.number_input(
            "çˆ¬å–é¡µæ•°",
            min_value=1,
            max_value=10,
            value=1,
            help="å»ºè®®ä»å°‘é‡é¡µé¢å¼€å§‹æµ‹è¯•"
        )

    with col2:
        st.write("**çˆ¬å–è®¾ç½®**")
        st.caption(f"æ•°æ®æº: {SOURCES_LABELS.get(selected_source, selected_source)}")
        st.caption(f"æœ€å¤§é¡µæ•°: {max_pages}")

    st.divider()

    # Quick actions
    st.write("**å¿«é€Ÿæ“ä½œ**")

    quick_col1, quick_col2, quick_col3 = st.columns(3)

    with quick_col1:
        if st.button("ğŸ“˜ çˆ¬å–çŸ¥ä¹", use_container_width=True):
            _trigger_crawl('zhihu', 1)

    with quick_col2:
        if st.button("ğŸ“° çˆ¬å–ä»Šæ—¥å¤´æ¡", use_container_width=True):
            _trigger_crawl('toutiao', 1)

    with quick_col3:
        if st.button("ğŸ’¬ çˆ¬å–å¾®ä¿¡", use_container_width=True):
            _trigger_crawl('wechat', 1)

    st.divider()

    # Custom crawl
    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("ğŸš€ å¼€å§‹çˆ¬å–", type="primary", use_container_width=True):
            _trigger_crawl(selected_source, max_pages)

    with col2:
        if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", use_container_width=True):
            st.rerun()

    with col3:
        if st.button("ğŸ“Š æŸ¥çœ‹ç»Ÿè®¡", use_container_width=True):
            stats = backend_api.get_statistics()
            if 'error' not in stats:
                st.json(stats)


def _show_crawl_logs():
    """Show crawl execution logs."""
    st.subheader("çˆ¬è™«æ‰§è¡Œè®°å½•")

    # Get logs
    with st.spinner("åŠ è½½æ—¥å¿—..."):
        logs = database_api.get_crawl_logs(limit=100)

    if not logs:
        st.info("æš‚æ— çˆ¬è™«æ‰§è¡Œè®°å½•")
        return

    # Summary chart
    st.write("**æœ€è¿‘æ‰§è¡Œè®°å½•**")
    crawl_logs_chart(logs)

    st.divider()

    # Detailed logs
    st.write("**è¯¦ç»†è®°å½•**")

    for log in logs:
        with st.expander(
            f"ğŸ“… {log['start_time']} | {SOURCES_LABELS.get(log['source'], log['source'])} | "
            f"âœ… {log['success_count']} | âŒ {log['failed_count']}"
        ):
            col1, col2 = st.columns(2)

            with col1:
                st.write("**åŸºæœ¬ä¿¡æ¯**")
                st.write(f"- æ•°æ®æº: {SOURCES_LABELS.get(log['source'], log['source'])}")
                st.write(f"- å¼€å§‹æ—¶é—´: {log['start_time']}")
                if log['end_time']:
                    from utils.helpers import format_time
                    st.write(f"- ç»“æŸæ—¶é—´: {log['end_time']}")
                    # Calculate duration
                    try:
                        from datetime import datetime
                        start = datetime.fromisoformat(log['start_time'])
                        end = datetime.fromisoformat(log['end_time'])
                        duration = (end - start).total_seconds()
                        st.write(f"- æ‰§è¡Œæ—¶é•¿: {duration:.1f} ç§’")
                    except:
                        pass

            with col2:
                st.write("**æ‰§è¡Œç»“æœ**")
                st.metric("æˆåŠŸ", log['success_count'])
                st.metric("å¤±è´¥", log['failed_count'])

                if log['error_msg']:
                    st.error(f"é”™è¯¯ä¿¡æ¯: {log['error_msg']}")


def _trigger_crawl(source: str, max_pages: int):
    """Trigger a crawl job."""
    with st.spinner(f"æ­£åœ¨çˆ¬å– {source}..."):
        result = backend_api.trigger_crawl(source=source, max_pages=max_pages)

    if 'error' in result:
        st.error(f"çˆ¬å–å¤±è´¥: {result.get('error')}")
    else:
        st.success(f"çˆ¬å–å®Œæˆï¼")
        st.json(result)

        # Show stats
        if isinstance(result, dict):
            stats_col1, stats_col2, stats_col3 = st.columns(3)
            with stats_col1:
                st.metric("æˆåŠŸ", result.get('success', 0))
            with stats_col2:
                st.metric("å¤±è´¥", result.get('failed', 0))
            with stats_col3:
                st.metric("æ€»è®¡", result.get('total', 0))

    st.rerun()
