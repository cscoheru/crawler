# 错误处理与降级方案

## 1. 概述

本文档定义爬虫数据管理与内容创作系统的错误处理策略、降级方案和故障恢复机制。

---

## 2. 错误分类

### 2.1 按严重程度分类

| 级别 | 描述 | 示例 | 处理策略 |
|------|------|------|---------|
| P0 - 致命 | 系统无法运行 | 数据库连接失败 | 立即告警，自动重启 |
| P1 - 严重 | 核心功能不可用 | AI 服务不可用 | 降级方案，告警 |
| P2 - 警告 | 部分功能受影响 | 单个爬虫失败 | 重试，记录日志 |
| P3 - 信息 | 不影响功能 | 重复数据 | 仅记录日志 |

### 2.2 按组件分类

| 组件 | 错误类型 | 降级方案 |
|------|---------|---------|
| 爬虫层 | 超时、反爬虫、API 变化 | 重试、代理池、降级到其他源 |
| 分类层 | AI 超时、置信度低 | 降级到规则分类 |
| 存储层 | 连接失败、查询超时 | 重试、缓存降级 |
| 外部服务 | Dify 不可用、AI 不可用 | 本地备份、降级功能 |

---

## 3. 外部服务降级方案

### 3.1 Pinecone (向量检索) 不可用

**降级策略**:

```python
class VectorSearchService:
    def __init__(self):
        self.pinecone_enabled = True
        self.fallback_to_keyword = True
        self.local_vector_store = LocalVectorStore()  # 本地备份

    async def search_similar_articles(self, topic: str, limit: int = 10):
        try:
            # 1. 尝试使用 Pinecone 向量搜索
            if self.pinecone_enabled:
                return await self._pinecone_search(topic, limit)
        except Exception as e:
            logger.warning(f"Pinecone search failed: {e}")

        # 2. 降级到本地向量库
        try:
            return await self._local_vector_search(topic, limit)
        except Exception as e:
            logger.warning(f"Local vector search failed: {e}")

        # 3. 降级到关键词搜索
        if self.fallback_to_keyword:
            logger.info("Falling back to keyword search")
            return await self._keyword_search(topic, limit)

        # 4. 最后降级到数据库全文搜索
        return await self._database_fulltext_search(topic, limit)
```

**本地向量库备份 (ChromaDB)**:

```python
import chromadb

class LocalVectorStore:
    def __init__(self, persist_directory="./data/vectors"):
        self.client = chromadb.PersistentClient(path=persist_directory)
        self.collection = self.client.get_or_create_collection("articles")

    def add_vectors(self, embeddings, documents, ids):
        self.collection.add(embeddings=embeddings, documents=documents, ids=ids)

    def search(self, query_embedding, n_results=10):
        return self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
```

### 3.2 Supabase (数据库) 连接失败

**重试策略**:

```python
class SupabaseClient:
    def __init__(self):
        self.max_retries = 3
        self.retry_delay = 1  # 秒
        self.circuit_breaker_threshold = 5
        self.circuit_breaker_failures = 0
        self.circuit_breaker_open = False

    async def execute_with_retry(self, operation):
        """带重试和熔断器的操作执行"""
        # 检查熔断器
        if self.circuit_breaker_open:
            raise Exception("Circuit breaker is open")

        last_error = None

        for attempt in range(self.max_retries):
            try:
                result = await operation()
                # 成功后重置熔断器
                self.circuit_breaker_failures = 0
                self.circuit_breaker_open = False
                return result
            except Exception as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    # 指数退避
                    wait_time = self.retry_delay * (2 ** attempt)
                    logger.warning(f"Attempt {attempt + 1} failed, retrying in {wait_time}s...")
                    await asyncio.sleep(wait_time)

        # 触发熔断器
        self.circuit_breaker_failures += 1
        if self.circuit_breaker_failures >= self.circuit_breaker_threshold:
            self.circuit_breaker_open = True
            logger.error("Circuit breaker opened due to repeated failures")

        raise last_error
```

**本地缓存降级**:

```python
class DatabaseManagerWithCache:
    def __init__(self):
        self.supabase_client = SupabaseClient()
        self.local_cache = {}  # 或使用 Redis
        self.cache_ttl = 300  # 5分钟

    async def get_articles(self, filters: dict):
        cache_key = hashlib.md5(json.dumps(filters).encode()).hexdigest()

        # 尝试从缓存读取
        if cache_key in self.local_cache:
            cached_data, timestamp = self.local_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                logger.info("Returning cached data")
                return cached_data

        # 尝试从 Supabase 读取
        try:
            articles = await self.supabase_client.get_articles(filters)
            self.local_cache[cache_key] = (articles, time.time())
            return articles
        except Exception as e:
            logger.error(f"Supabase error: {e}")
            # 返回缓存数据（即使过期）
            if cache_key in self.local_cache:
                logger.warning("Returning stale cached data")
                return self.local_cache[cache_key][0]
            raise
```

### 3.3 AI 服务 (GLM/DeepSeek) 超时

**超时处理与降级**:

```python
class AIClassifier:
    def __init__(self):
        self.timeout = 30
        self.max_retries = 3

    async def classify_with_timeout(self, title: str, content: str):
        for attempt in range(self.max_retries):
            try:
                async with asyncio.timeout(self.timeout):
                    return await self._call_ai_api(title, content)
            except asyncio.TimeoutError:
                logger.warning(f"AI API timeout, attempt {attempt + 1}/{self.max_retries}")
                if attempt < self.max_retries - 1:
                    continue

                # 所有尝试失败，降级到规则分类
                logger.warning("All AI attempts failed, falling back to rule-based")
                return await self._fallback_to_rules(title, content)

    async def _fallback_to_rules(self, title: str, content: str):
        """降级到规则分类"""
        from classifier.rule_based import RuleBasedClassifier
        classifier = RuleBasedClassifier()
        result = classifier.classify(title, content)
        return {
            **result,
            "method": "rule_based_fallback",
            "confidence": result["confidence"] * 0.8  # 降低置信度
        }
```

### 3.4 Dify 知识库不可用

**降级策略**:

```python
class DifySyncer:
    def __init__(self):
        self.max_retries = 3
        self.queue_file = "./data/dify_queue.json"

    async def sync_article(self, article: dict):
        for attempt in range(self.max_retries):
            try:
                return await self._upload_to_dify(article)
            except Exception as e:
                logger.warning(f"Dify sync failed (attempt {attempt + 1}): {e}")

        # 所有重试失败，加入待同步队列
        logger.warning("Adding article to sync queue")
        await self._add_to_queue(article)
        return {"success": False, "queued": True}

    async def _add_to_queue(self, article: dict):
        """添加到待同步队列"""
        queue_data = []
        if os.path.exists(self.queue_file):
            with open(self.queue_file, 'r') as f:
                queue_data = json.load(f)

        queue_data.append({
            "article": article,
            "timestamp": time.time()
        })

        with open(self.queue_file, 'w') as f:
            json.dump(queue_data, f)

    async def process_queue(self):
        """定期处理队列"""
        if not os.path.exists(self.queue_file):
            return

        with open(self.queue_file, 'r') as f:
            queue_data = json.load(f)

        remaining = []
        for item in queue_data:
            try:
                await self._upload_to_dify(item["article"])
            except:
                remaining.append(item)

        with open(self.queue_file, 'w') as f:
            json.dump(remaining, f)
```

---

## 4. 爬虫错误处理

### 4.1 爬虫请求失败

**失败记录与告警**:

```python
class CrawlerWithAlerts:
    def __init__(self):
        self.failure_threshold = 10  # 连续失败阈值
        self.alert_cooldown = 3600   # 告警冷却时间 (秒)
        self.last_alert_time = {}

    async def crawl_with_alerts(self, source: str, keywords: list):
        failure_count = 0

        for keyword in keywords:
            try:
                articles = await self._crawl_keyword(source, keyword)
                failure_count = 0  # 重置
                yield articles

            except Exception as e:
                failure_count += 1
                logger.error(f"Failed to crawl {keyword}: {e}")

                # 记录失败
                await self._log_crawl_failure(source, keyword, str(e))

                # 触发告警
                if failure_count >= self.failure_threshold:
                    await self._send_alert(source, failure_count)
```

**告警实现**:

```python
async def _send_alert(self, source: str, failure_count: int):
    """发送告警通知"""
    alert_key = f"{source}_crawl_failure"
    current_time = time.time()

    # 检查冷却时间
    if alert_key in self.last_alert_time:
        if current_time - self.last_alert_time[alert_key] < self.alert_cooldown:
            return

    # 发送告警
    message = f"⚠️ 爬虫告警: {source} 连续失败 {failure_count} 次"

    # 支持多种告警方式
    await self._send_wechat_alert(message)
    await self._send_email_alert(message)

    self.last_alert_time[alert_key] = current_time
```

### 4.2 反爬虫处理

```python
class AntiSpiderHandler:
    def __init__(self):
        self.blocked_ips = set()
        self.blocked_user_agents = set()

    async def handle_anti_spider(self, response, source: str):
        """处理反爬虫响应"""
        if response.status_code == 403:
            logger.warning(f"Access forbidden for {source}")
            # 切换代理
            await self._rotate_proxy()
            return True

        if response.status_code == 429:
            logger.warning(f"Rate limited for {source}")
            # 增加延迟
            await self._increase_delay()
            return True

        if "captcha" in response.text.lower():
            logger.warning(f"Captcha detected for {source}")
            # 标记需要人工处理
            await self._mark_for_manual_review(source)
            return True

        return False
```

---

## 5. 任务故障恢复

### 5.1 任务持久化

```python
class PersistentTaskQueue:
    def __init__(self, db_manager):
        self.db = db_manager

    async def add_task(self, task_type: str, task_data: dict):
        """添加任务到持久化队列"""
        task = {
            "id": str(uuid.uuid4()),
            "type": task_type,
            "data": task_data,
            "status": "pending",
            "created_at": datetime.now(),
            "attempts": 0
        }

        # 保存到数据库
        await self.db.save_task(task)
        return task["id"]

    async def get_pending_tasks(self):
        """获取待处理任务"""
        tasks = await self.db.get_tasks_by_status("pending")

        # 检查超时任务
        timeout_threshold = datetime.now() - timedelta(minutes=30)
        timeout_tasks = await self.db.get_timeout_tasks(timeout_threshold)

        return tasks + timeout_tasks
```

### 5.2 任务恢复机制

```python
async def recover_tasks_on_startup():
    """服务启动时恢复未完成的任务"""
    task_queue = PersistentTaskQueue(db_manager)

    # 获取所有未完成的任务
    pending_tasks = await task_queue.get_pending_tasks()

    logger.info(f"Found {len(pending_tasks)} pending tasks to recover")

    for task in pending_tasks:
        try:
            # 重新执行任务
            await execute_task(task)
        except Exception as e:
            logger.error(f"Failed to recover task {task['id']}: {e}")
```

---

## 6. Railway 服务重启保护

### 6.1 健康检查优化

```python
@app.route('/health')
def health():
    """增强的健康检查"""
    health_status = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }

    # 检查数据库连接
    try:
        db_manager.get_session().__enter__()
        health_status["database"] = "healthy"
    except:
        health_status["database"] = "unhealthy"
        health_status["status"] = "degraded"

    # 检查外部服务
    if os.getenv('DIFY_API_KEY'):
        try:
            response = requests.get(f"{os.getenv('DIFY_BASE_URL')}/health", timeout=5)
            health_status["dify"] = "healthy" if response.status_code == 200 else "unhealthy"
        except:
            health_status["dify"] = "unreachable"

    # 根据状态返回不同 HTTP 状态码
    status_code = 200 if health_status["status"] == "healthy" else 503

    return jsonify(health_status), status_code
```

### 6.2 优雅关闭

```python
import signal
import asyncio

class GracefulShutdown:
    def __init__(self):
        self.shutdown = False

    def init(self):
        signal.signal(signal.SIGTERM, self.handle_shutdown)
        signal.signal(signal.SIGINT, self.handle_shutdown)

    def handle_shutdown(self, signum, frame):
        logger.info(f"Received signal {signum}, initiating graceful shutdown...")
        self.shutdown = True

        # 保存未完成的任务
        asyncio.create_task(self.save_pending_tasks())

    async def save_pending_tasks(self):
        """保存未完成的任务"""
        # 实现任务持久化逻辑
        pass
```

---

## 7. 错误日志与监控

### 7.1 结构化日志

```python
import structlog

logger = structlog.get_logger()

# 使用示例
logger.info(
    "article_saved",
    article_id=123,
    source="zhihu",
    category="psychology"
)

logger.error(
    "crawl_failed",
    source="zhihu",
    keyword="心理咨询",
    error="Connection timeout",
    retry_count=3
)
```

### 7.2 错误追踪

```python
import sentry_sdk

sentry_sdk.init(
    dsn=os.getenv("SENTRY_DSN"),
    traces_sample_rate=0.1,
    profiles_sample_rate=0.1
)

# 自动捕获未处理异常
@app.errorhandler(Exception)
def handle_exception(e):
    sentry_sdk.capture_exception(e)
    return jsonify({"error": "Internal server error"}), 500
```

---

## 8. 故障演练建议

| 演练场景 | 频率 | 验证点 |
|---------|------|-------|
| 数据库连接中断 | 每月 | 重试机制、缓存降级 |
| AI 服务超时 | 每月 | 降级到规则分类 |
| Dify 不可用 | 每月 | 队列机制、后续重试 |
| Railway 重启 | 每次部署 | 任务恢复机制 |
| 爬虫大规模失败 | 每月 | 告警机制 |

---

**文档版本**: v1.0
**最后更新**: 2024-02-26
