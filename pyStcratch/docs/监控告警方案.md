# 监控告警方案

## 1. 概述

本文档定义爬虫数据管理与内容创作系统的监控指标、告警策略和监控工具配置。

---

## 2. 监控指标体系

### 2.1 系统层监控

| 指标 | 类型 | 告警阈值 | 采集频率 |
|------|------|---------|---------|
| CPU 使用率 | Gauge | >80% 警告, >95% 严重 | 10s |
| 内存使用率 | Gauge | >85% 警告, >95% 严重 | 10s |
| 磁盘使用率 | Gauge | >90% 警告 | 60s |
| 网络连接数 | Gauge | >1000 警告 | 30s |
| 磁盘 I/O | Gauge | >80% 警告 | 30s |

### 2.2 应用层监控

| 指标 | 类型 | 告警阈值 | 采集频率 |
|------|------|---------|---------|
| API 响应时间 P95 | Histogram | >3s 警告, >5s 严重 | 实时 |
| API 响应时间 P99 | Histogram | >5s 警告, >10s 严重 | 实时 |
| API 错误率 | Gauge | >5% 警告, >10% 严重 | 实时 |
| 请求速率 | Counter | - | 实时 |
| 活跃任务数 | Gauge | >100 警告 | 30s |
| 数据库连接数 | Gauge | >80% 池大小 警告 | 30s |

### 2.3 业务层监控

| 指标 | 类型 | 告警阈值 | 采集频率 |
|------|------|---------|---------|
| 日新增文章数 | Gauge | <10 警告 | 每小时 |
| 爬虫成功率 | Gauge | <70% 警告 | 每次爬取 |
| AI 创作失败率 | Gauge | >20% 警告 | 实时 |
| Dify 同步失败率 | Gauge | >10% 警告 | 实时 |
| 文章平均质量分 | Gauge | <0.5 警告 | 每天 |
| 垃圾文章比例 | Gauge | >10% 警告 | 每天 |

### 2.4 自定义业务指标

```python
from prometheus_client import Counter, Histogram, Gauge

# 定义指标
article_crawled = Counter('articles_crawled_total', 'Total articles crawled', ['source', 'status'])
article_quality = Gauge('article_quality_score', 'Article quality score', ['category'])
crawl_duration = Histogram('crawl_duration_seconds', 'Crawl duration', ['source'])
ai_creation_duration = Histogram('ai_creation_duration_seconds', 'AI creation duration')

# 使用示例
article_crawled.labels(source='zhihu', status='success').inc()
article_quality.labels(category='psychology').set(0.75)
```

---

## 3. 监控工具配置

### 3.1 Prometheus 配置

**prometheus.yml**:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'crawler-service'
    static_configs:
      - targets: ['crawler-service:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```

### 3.2 Grafana 仪表盘

**推荐面板**:

1. **系统概览**
   - CPU 使用率趋势
   - 内存使用率趋势
   - 磁盘使用率
   - 网络流量

2. **API 性能**
   - 请求速率 (QPS)
   - 响应时间分布 (P50, P95, P99)
   - 错误率
   - 端点状态

3. **业务指标**
   - 文章总数趋势
   - 各平台文章分布
   - 分类分布
   - 质量分数分布

4. **任务状态**
   - 爬虫任务状态
   - 创作任务状态
   - 队列积压情况

**Dashboard JSON 示例**:
```json
{
  "dashboard": {
    "title": "爬虫系统监控",
    "panels": [
      {
        "title": "API 响应时间",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(api_request_duration_seconds_bucket[5m]))"
          }
        ]
      },
      {
        "title": "文章总数",
        "targets": [
          {
            "expr": "articles_total"
          }
        ]
      }
    ]
  }
}
```

### 3.3 中间件集成

**Flask Prometheus 中间件**:
```python
from prometheus_client import Counter, Histogram, make_wsgi_app
from werkzeug.middleware.dispatcher import DispatcherMiddleware

# 指标定义
request_count = Counter('api_requests_total', 'Total API requests',
                        ['method', 'endpoint', 'status'])
request_duration = Histogram('api_request_duration_seconds', 'API request duration')

# 中间件
@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()

    response = await call_next(request)

    # 记录指标
    request_count.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()

    duration = time.time() - start_time
    request_duration.observe(duration)

    return response

# 挂载 /metrics 端点
app.mount("/metrics", make_wsgi_app())
```

---

## 4. 告警策略

### 4.1 告警规则 (Prometheus)

**alerting_rules.yml**:
```yaml
groups:
  - name: system_alerts
    interval: 30s
    rules:
      # CPU 告警
      - alert: HighCPUUsage
        expr: rate(process_cpu_seconds_total[5m]) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% for 5 minutes"

      - alert: CriticalCPUUsage
        expr: rate(process_cpu_seconds_total[5m]) > 0.95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical CPU usage detected"
          description: "CPU usage is above 95% for 2 minutes"

      # 内存告警
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / os_total_memory > 0.85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"

  - name: api_alerts
    interval: 30s
    rules:
      # API 响应时间告警
      - alert: HighAPILatency
        expr: histogram_quantile(0.95, rate(api_request_duration_seconds_bucket[5m])) > 3
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High API latency detected"
          description: "P95 latency is above 3 seconds"

      # API 错误率告警
      - alert: HighErrorRate
        expr: rate(api_requests_total{status=~"5.."}[5m]) / rate(api_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5%"

  - name: business_alerts
    interval: 1m
    rules:
      # 文章新增告警
      - alert: LowArticleCreation
        expr: increase(articles_crawled_total[1h]) < 10
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "Low article creation rate"
          description: "Less than 10 articles created in the last hour"

      # 爬虫失败率告警
      - alert: HighCrawlFailureRate
        expr: rate(articles_crawled_total{status="failed"}[10m]) / rate(articles_crawled_total[10m]) > 0.3
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High crawl failure rate"
          description: "Crawl failure rate is above 30%"
```

### 4.2 告警路由 (Alertmanager)

**alertmanager.yml**:
```yaml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'

  routes:
    # 严重告警立即发送
    - match:
        severity: critical
      receiver: 'critical'
      group_wait: 10s
      repeat_interval: 5m

    # 警告级别告警
    - match:
        severity: warning
      receiver: 'warning'
      group_wait: 5m
      repeat_interval: 1h

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'

  - name: 'critical'
    slack_configs:
      - channel: '#critical-alerts'
    webhook_configs:
      - url: 'https://your-wechat-webhook-url'

  - name: 'warning'
    slack_configs:
      - channel: '#alerts'
```

---

## 5. 告警通知方式

### 5.1 邮件告警

```python
import smtplib
from email.mime.text import MIMEText

class EmailAlerter:
    def __init__(self):
        self.smtp_server = os.getenv("SMTP_SERVER")
        self.smtp_port = int(os.getenv("SMTP_PORT", "587"))
        self.smtp_username = os.getenv("SMTP_USERNAME")
        self.smtp_password = os.getenv("SMTP_PASSWORD")
        self.recipients = os.getenv("ALERT_EMAILS", "").split(",")

    async def send_alert(self, subject: str, message: str):
        """发送邮件告警"""
        msg = MIMEText(message)
        msg['Subject'] = f"[Alert] {subject}"
        msg['From'] = self.smtp_username
        msg['To'] = ", ".join(self.recipients)

        with smtplib.SMTP(self.smtp_server, self.smtp_port) as server:
            server.starttls()
            server.login(self.smtp_username, self.smtp_password)
            server.send_message(msg)
```

### 5.2 微信/钉钉告警

**微信企业号**:
```python
class WeChatAlerter:
    def __init__(self):
        self.webhook_url = os.getenv("WECHAT_WEBHOOK_URL")

    async def send_alert(self, message: str):
        """发送微信告警"""
        if not self.webhook_url:
            return

        payload = {
            "msgtype": "text",
            "text": {
                "content": f"⚠️ 系统告警:\n{message}"
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(self.webhook_url, json=payload) as response:
                return await response.json()
```

**钉钉**:
```python
class DingTalkAlerter:
    def __init__(self):
        self.webhook_url = os.getenv("DINGTALK_WEBHOOK_URL")
        self.secret = os.getenv("DINGTALK_SECRET")

    async def send_alert(self, title: str, message: str):
        """发送钉钉告警"""
        import time
        import hmac
        import hashlib
        import base64
        import urllib.parse

        # 生成签名
        timestamp = str(round(time.time() * 1000))
        secret_enc = self.secret.encode('utf-8')
        string_to_sign = f'{timestamp}\n{self.secret}'
        string_to_sign_enc = string_to_sign.encode('utf-8')
        hmac_code = hmac.new(secret_enc, string_to_sign_enc, digestmod=hashlib.sha256).digest()
        sign = urllib.parse.quote_plus(base64.b64encode(hmac_code))

        # 发送请求
        url = f"{self.webhook_url}&timestamp={timestamp}&sign={sign}"
        payload = {
            "msgtype": "markdown",
            "markdown": {
                "title": title,
                "text": f"### {title}\n{message}"
            }
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload) as response:
                return await response.json()
```

### 5.3 Slack 告警

```python
class SlackAlerter:
    def __init__(self):
        self.webhook_url = os.getenv("SLACK_WEBHOOK_URL")

    async def send_alert(self, title: str, message: str, color: str = "danger"):
        """发送 Slack 告警"""
        payload = {
            "attachments": [
                {
                    "color": color,
                    "title": title,
                    "text": message,
                    "footer": "Crawler System Alerts",
                    "ts": int(time.time())
                }
            ]
        }

        async with aiohttp.ClientSession() as session:
            async with session.post(self.webhook_url, json=payload) as response:
                return await response.json()
```

---

## 6. 日志聚合

### 6.1 ELK Stack 集成

**Filebeat 配置**:
```yaml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /logs/crawler.log
    fields:
      service: crawler
    fields_under_root: true

output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  indices:
    - index: "crawler-logs-%{+yyyy.MM.dd}"

setup.template.name: "crawler-logs"
setup.template.pattern: "crawler-logs-*"
```

### 6.2 Loki + Grafana

**Promtail 配置**:
```yaml
server:
  http_listen_port: 9080

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  - job_name: crawler-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: crawler
          __path__: /logs/*.log
```

---

## 7. 告警抑制与聚合

### 7.1 告警抑制规则

```yaml
inhibit_rules:
  # 如果 CPU 和内存都告警，只发送严重级别的
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']

  # 如果服务已下线，抑制该服务的所有其他告警
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*'
    equal: ['service']
```

### 7.2 告警聚合窗口

```yaml
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s      # 等待 10s 收集同组告警
  group_interval: 10s  # 同组告警间隔 10s
  repeat_interval: 12h # 12 小时后重复告警
```

---

## 8. 监控最佳实践

### 8.1 SMART 原则

| 原则 | 说明 | 示例 |
|------|------|------|
| Specific | 具体明确 | "CPU 使用率 > 80%" |
| Measurable | 可度量 | 使用数值指标 |
| Achievable | 可实现 | 阈值设置合理 |
| Relevant | 相关性 | 监控核心业务指标 |
| Time-bound | 时间限制 | "持续 5 分钟" |

### 8.2 告警级别定义

| 级别 | 响应时间 | 处理优先级 | 示例 |
|------|---------|-----------|------|
| P0 - 致命 | 立即 | 最高 | 服务宕机 |
| P1 - 严重 | 15分钟 | 高 | 数据库连接失败 |
| P2 - 警告 | 1小时 | 中 | API 响应慢 |
| P3 - 信息 | 次日 | 低 | 日志异常 |

### 8.3 告警疲劳预防

1. **合并相似告警**: 使用告警聚合
2. **设置合理阈值**: 避免误报
3. **使用告警冷却**: 同类告警间隔发送
4. **定期回顾**: 每月回顾告警有效性

---

## 9. 故障响应流程

### 9.1 故障等级响应

| 等级 | 响应时间 | 解决时间 | 通知范围 |
|------|---------|---------|---------|
| P0 | 5分钟 | 1小时 | 全员 + 管理层 |
| P1 | 15分钟 | 4小时 | 开发团队 |
| P2 | 1小时 | 24小时 | 责任人 |
| P3 | 次日 | 3天 | 责任人 |

### 9.2 故障处理流程

```
告警触发 → 确认故障 → 定位问题 → 执行修复 → 验证恢复 → 复盘总结
    ↓         ↓         ↓         ↓         ↓         ↓
  自动     自动     人工     人工     自动     人工
  告警     确认     排查     修复     验证     记录
```

---

**文档版本**: v1.0
**最后更新**: 2024-02-26
